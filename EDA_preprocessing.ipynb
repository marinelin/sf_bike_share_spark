{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: java.io.IOException: No FileSystem for scheme: s3\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:258)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bc6d93c0a741>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrip_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's3://msds694.proj/data/trip.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtrips_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrip_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrips_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# Removing the header row from the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrips_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DistributedComputing/lib/python3.7/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DistributedComputing/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DistributedComputing/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: java.io.IOException: No FileSystem for scheme: s3\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:258)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "# Akanksha's Preprocessing\n",
    "\n",
    "import datetime\n",
    "# Reading trip.csv from s3 cluster\n",
    "trip_rdd = sc.textFile('s3://msds694.proj/data/trip.csv', 24)\n",
    "trips_rdd = trip_rdd.map(lambda x: x.split(\",\"))\n",
    "trips = trips_rdd.collect()\n",
    "# Removing the header row from the data\n",
    "header = trips_rdd.first()\n",
    "trips_rdd_rows = trips_rdd.filter(lambda line: line != header)\n",
    "# Creating an RDD with only Subscriber data\n",
    "trips_rdd_rows_subscribers = trips_rdd_rows.filter(lambda x: x[9] != 'Customer')\n",
    "# Creating an RDD with only Customer data\n",
    "trips_rdd_rows_customers = trips_rdd_rows.filter(lambda x: x[9] != 'Subscriber')\n",
    "# Getting the day of the week from the date column\n",
    "# Creating a key-value pair with day of the week as the key, and 1 as the value (to keep track of bike trips)\n",
    "trips_subscriber_dow_subtype = trips_rdd_rows_subscribers.map(lambda x: ((datetime.datetime.strptime(x[2], \n",
    "                                       '%m/%d/%Y %H:%M').strftime(\"%A\")), 1))\n",
    "trips_customer_dow_subtype = trips_rdd_rows_customers.map(lambda x: ((datetime.datetime.strptime(x[2], \n",
    "                                       '%m/%d/%Y %H:%M').strftime(\"%A\")), 1))\n",
    "# GroupByKey and sum of values will fetch the number of trips on a given day of week\n",
    "# For Subscribers:\n",
    "trips_sub_dow_subtype_grouped = trips_subscriber_dow_subtype.groupByKey().mapValues(lambda x: sum(x))\n",
    "# For Customers:\n",
    "trips_cus_dow_subtype_grouped = trips_customer_dow_subtype.groupByKey().mapValues(lambda x: sum(x))\n",
    "# Save for EDA\n",
    "trips_sub_dow_subtype_grouped.saveAsTextFile('s3://msds694.proj/data/sub_total_trip_by_dow')\n",
    "trips_cus_dow_subtype_grouped.saveAsTextFile('s3://msds694.proj/data/cus_total_trip_by_dow')\n",
    "# Getting the day of the week from the date column\n",
    "# Creating a key-value pair with day of the week as the key, and duration of the trip as the value\n",
    "trips_sub_dow_duration = trips_rdd_rows_subscribers.map(lambda x: ((datetime.datetime.strptime(x[2], \n",
    "                                       '%m/%d/%Y %H:%M').strftime(\"%A\")), int(x[1])))\n",
    "trips_cus_dow_duration = trips_rdd_rows_customers.map(lambda x: ((datetime.datetime.strptime(x[2], \n",
    "                                       '%m/%d/%Y %H:%M').strftime(\"%A\")), int(x[1])))\n",
    "# GroupByKey and sum of values will fetch the total duration of trips (in mins) on a given day of week\n",
    "# For Subscribers:\n",
    "trips_sub_dow_duration_sum = trips_sub_dow_duration.groupByKey().mapValues(lambda x: round(sum(x)/60, 2))\n",
    "# For Customers:\n",
    "trips_cus_dow_duration_sum = trips_cus_dow_duration.groupByKey().mapValues(lambda x: round(sum(x)/60, 2))\n",
    "# Getting the day of the week from the date column\n",
    "# Creating a key-value pair with day of the week as the key, and id of the trip as the value\n",
    "trips_sub_dow_id = trips_rdd_rows_subscribers.map(lambda x: ((datetime.datetime.strptime(x[2], \n",
    "                                       '%m/%d/%Y %H:%M').strftime(\"%A\")), x[0]))\n",
    "trips_cus_dow_id = trips_rdd_rows_customers.map(lambda x: ((datetime.datetime.strptime(x[2], \n",
    "                                       '%m/%d/%Y %H:%M').strftime(\"%A\")), x[0]))\n",
    "# GroupByKey and count of ids will fetch the total trips on a given day of week\n",
    "# For Subscribers:\n",
    "trips_sub_dow_id_count = trips_sub_dow_id.groupByKey().mapValues(lambda x: len(set(x)))\n",
    "# For Customers:\n",
    "trips_cus_dow_id_count = trips_cus_dow_id.groupByKey().mapValues(lambda x: len(set(x)))\n",
    "# Join the RDD with (Day of week, total trip duration) as key-value pair, to the\n",
    "# RDD with (Day of week, total trips) as key-value pair on key -> Day of Week\n",
    "trips_sub_dow_id_ct_dur = trips_sub_dow_duration_sum.join(trips_sub_dow_id_count)\n",
    "trips_cus_dow_id_ct_dur = trips_cus_dow_duration_sum.join(trips_cus_dow_id_count)\n",
    "# Compute the average duration per trip per day of week for both Subscribers and the Customers\n",
    "trips_sub_dow_avg_dur = trips_sub_dow_id_ct_dur.mapValues(lambda x: x[0]/x[1])\n",
    "trips_cus_dow_avg_dur = trips_cus_dow_id_ct_dur.mapValues(lambda x: x[0]/x[1])\n",
    "# Save for EDA\n",
    "trips_sub_dow_avg_dur.saveAsTextFile('s3://msds694.proj/data/sub_avg_trip_duration_by_dow')\n",
    "trips_cus_dow_avg_dur.saveAsTextFile('s3://msds694.proj/data/cus_avg_trip_duration_by_dow')\n",
    "\n",
    "\n",
    "# Esther's preprocessing\n",
    "\n",
    "rdd1= sc.textFile('s3://msds694.proj/data/trip.csv',8)\n",
    "rdd1.cache()\n",
    "rdd1.collect()\n",
    "lines1 = rdd1.map(lambda x: ((x.split(',')[2][:x.split(',')[2].find(' ')],x.split(',')[9]),\n",
    "                            (x.split(',')[0],x.split(',')[1])))\n",
    "lines1.cache()\n",
    "lines1.collect()\n",
    "pair1 = lines1.groupByKey().mapValues(lambda x:len(list(x)))\n",
    "pair1.cache()\n",
    "pair1.collect()\n",
    "pair2 = pair1.map(lambda x:(x[0][0],(x[0][1],x[1])))\n",
    "pair2.collect()\n",
    "rdd = sc.textFile('s3://msds694.proj/data/weather.csv',24)\n",
    "rdd.collect()\n",
    "lines2 = rdd.map(lambda x: (x.split(',')[0],\n",
    "                            (x.split(',')[2],x.split(',')[19],x.split(',')[23])))\n",
    "lines2.collect()\n",
    "lines3 = lines2.filter(lambda x:x[1][2]=='94107')\n",
    "joined = lines3.leftOuterJoin(pair2)\n",
    "joined.collect()\n",
    "temp = joined.map(lambda x:((x[1][0][0], x[1][1][0] ),x[1][1][1]))\n",
    "rain = joined.map(lambda x:((x[1][0][1], x[1][1][0] ),x[1][1][1]))\n",
    "temp_mean = temp.groupByKey().mapValues(lambda x:(len(list(x)),sum(list(x))/len(list(x))))\n",
    "rain_mean = rain.groupByKey().mapValues(lambda x:(len(list(x)),sum(list(x))/len(list(x))))\n",
    "temp_mean.saveAsTextFile(\"s3://msds694.proj/data/Esther/temp\")\n",
    "rain_mean.saveAsTextFile(\"s3://msds694.proj/data/Esther/rain\")\n",
    "\n",
    "\n",
    "# Marine's preprocessing\n",
    "\n",
    "station = sc.textFile('s3://msds694.proj/data/station.csv',24)\n",
    "station_c=station.map(lambda x:x.split(',')[0:4])\n",
    "trip_c=trip.map(lambda x:x.split(',')[1:8])\n",
    "key_pair=trip_c.map(lambda x: ((x[3],x[6]),1))\n",
    "start_end_count=key_pair.groupByKey().mapValues(lambda x : sum(x))\n",
    "start_end_count=start_end_count.map(lambda x: (x[0][0],x[0][1],x[1]))\n",
    "station_pair=station_c.map(lambda x: (x[0],(x[1],x[2],x[3])))\n",
    "start_key=start_end_count.map(lambda x:(x[0],(x[1],x[2])))\n",
    "df1=start_key.join(station_pair).map(lambda x: (x[0],x[1][0][0],x[1][0][1],x[1][1][0],x[1][1][1],x[1][1][2]))\n",
    "df2=df1.map(lambda x: (x[1],(x[0],x[2],x[3],x[4],x[5])))\n",
    "df_full=df2.join(station_pair).map(lambda x:(x[0],x[1][0][0],x[1][0][1],x[1][0][2],\n",
    "                                     x[1][0][3],x[1][0][4],x[1][1][0],x[1][1][1],x[1][1][2]))\n",
    "\n",
    "\n",
    "# Lexie's preprocessing\n",
    "\n",
    "rdd = sc.textFile('s3://msds694.proj/data/trip.csv', 24)\n",
    "rdd = rdd.flatMap(lambda x : [x.replace(';','').split(\",\")])\n",
    "rdd = rdd.filter(lambda x: x[0] != 'id')\n",
    "# subscriber\n",
    "subscriber_1 = rdd.filter(lambda x: x[9] == 'Subscriber')\n",
    "subscriber_1 = subscriber_1.map(lambda x: float(x[1])/60)\n",
    "subscriber_1 = subscriber_1.filter(lambda x: x<60)\n",
    "# customer\n",
    "customer_1 = rdd.filter(lambda x: x[9] == 'Customer')\n",
    "customer_1 = customer_1.map(lambda x: float(x[1])/60)\n",
    "customer_1 = customer_1.filter(lambda x: x<60)\n",
    "# save to S3\n",
    "subscriber_1.saveAsTextFile(\"s3://msds694.proj/data/Lexie/subscriber\")\n",
    "customer_1.saveAsTextFile(\"s3://msds694.proj/data/Lexie/customer\")\n",
    "\n",
    "# Kevin's preprocessing\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "from pyspark.sql.functions import udf, to_date, to_utc_timestamp\n",
    "import numpy as np\n",
    "\n",
    "station_path = 's3://msds694.proj/data/station.csv'\n",
    "status_path = 's3://msds694.proj/data/status.csv'\n",
    "trip_path = 's3://msds694.proj/data/trip.csv'\n",
    "weather_path = 's3://msds694.proj/data/weather.csv'\n",
    "status_rdd = sc.textFile(status_path)\n",
    "station_rdd = sc.textFile(station_path)\n",
    "status_header = status_rdd.take(1)\n",
    "station_header = station_rdd.take(1)\n",
    "status_header,station_header\n",
    "STATION_ID = 0\n",
    "BIKES_AVAILABLE = 1\n",
    "DOCKS_AVAILABLE = 2\n",
    "TIME = 3\n",
    "def change_col(x,col_num,func,**kwargs):\n",
    "    x[col_num] = func(x[col_num],**kwargs)\n",
    "    return x\n",
    "def makekey(x,*args):\n",
    "    key = tuple([item for idx,item in enumerate(x) if idx in args])\n",
    "    return (key,x)\n",
    "def flatten(x,iterations=1):\n",
    "    return sum(x[1])\n",
    "status_rdd_processed = (status_rdd.filter(lambda x: not x.startswith('station_id'))\n",
    "                                  .map(lambda x:x.split(','))\n",
    "                                  .map(lambda x:x[:TIME]+[x[TIME].replace('/','-')])\n",
    "                                  .map(lambda x: change_col(x,TIME,parse))\n",
    "                                  .filter(lambda x: x[TIME] >= datetime(2013,12,29) and x[TIME] <= datetime(2015,1,1))\n",
    "                                  .map(lambda x:x+[datetime(x[TIME].year,x[TIME].month,x[TIME].day,x[TIME].hour)]))\n",
    "status_rdd_processed.cache().count()\n",
    "status_rdd_processed.take(1)\n",
    "status_rdd_proccessed_key = status_rdd_processed.map(lambda x: makekey(x,STATION_ID))\n",
    "station_rdd_processed = (station_rdd.filter(lambda x: not x.startswith('id'))\n",
    "                                    .map(lambda x:x.split(','))\n",
    "                                    .filter(lambda x: x[5] == 'San Francisco')\n",
    "                                    .map(lambda x:makekey(x,0)))\n",
    "status_rdd_join_sf_station = status_rdd_proccessed_key.join(station_rdd_processed)\\\n",
    "                                                      .map(lambda x: x[1][0]+ x[1][1] ).cache()\n",
    "status_rdd_processed_gb_id_hour = status_rdd_join_sf_station.map(lambda x: makekey(x,4,6))\n",
    "min_timestamp_per_hour = status_rdd_processed_gb_id_hour.map(lambda x: (x[0],x[1][3]))\\\n",
    "                                                        .reduceByKey(lambda x,y:min(x,y)).cache()\n",
    "df = status_rdd_processed_gb_id_hour.join(min_timestamp_per_hour)\\\n",
    "                               .map(lambda x: x[1][0] + [x[1][1]])\\\n",
    "                               .filter(lambda x:x[3] == x[12])\\\n",
    "                               .map(lambda x:x[0:12]).cache()\n",
    "def is_weekend(date):\n",
    "    weekday_name = date.strftime('%A')\n",
    "    if weekday_name in ['Saturday','Sunday']:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "station_name_key = station_rdd_processed.map(lambda x:x[1])\\\n",
    "                                        .map(lambda x:makekey(x,1))\n",
    "df_agg = (df.map(lambda x: x+[is_weekend(x[4])])          # create weekend flag\n",
    "               .map(lambda x: x+[int(x[1])/int(x[9])])    # calculate percentate available\n",
    "               .map(lambda x: x+[x[4].hour])              # compute hour\n",
    "               .map(lambda x: [x[14],x[12],x[6],x[13]])   # select hour,weekend_flag,station_name,percent_bikes_avail\n",
    "               .map(lambda x: makekey(x,0,1,2))           # make hour,weekend_flag,and station_name keys\n",
    "               .map(lambda x:(x[0],x[1][-1]))             # grab only percent_bikes_avail column to be in values\n",
    "               .aggregateByKey((0,0), lambda a,b: (a[0] + b, a[1] + 1),  \n",
    "                                       lambda a,b: (a[0] + b[0], a[1] + b[1]))  # calculate a tuple of (cum_sum, count) to compute average\n",
    "               .mapValues(lambda x:x[0]/x[1])       # compute average pct_avail for each key\n",
    "               .map(lambda x: list(x[0])+[x[1]])    # flatten key value pairs\n",
    "               .map(lambda x: makekey(x,2))         # make station name the key\n",
    "               .join(station_name_key)              # join with station table to get info about station\n",
    "               .map(lambda x: x[1][0]+x[1][1]))     # flatten after join\n",
    "def toCSVLine(data):\n",
    "  return ','.join(str(d) for d in data)\n",
    "lines = df_agg.map(toCSVLine)\n",
    "lines.saveAsTextFile('s3://msds694.proj/data/marine/test_data_kevin_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
